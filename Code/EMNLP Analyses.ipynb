{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0baab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import ttest_ind, ttest_rel, ttest_1samp, chi2_contingency, spearmanr, pearsonr\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import json\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "pd.options.mode.chained_assignment = None  # default='warn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ab9945",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"../Data/\"\n",
    "PRE_LIWC_DIR = DATA_DIR + \"all_subreddit_data_pre_liwc/\"\n",
    "LIWC_DIR = DATA_DIR + \"liwc_data/\"\n",
    "TOXICITY_DIR = DATA_DIR + \"toxicity_scores/\"\n",
    "TRAINING_DIR = DATA_DIR + \"training_data/\"\n",
    "TESTING_DIR = DATA_DIR + \"testing_data/\"\n",
    "PREDICTIONS_DIR = DATA_DIR + \"regression_predictions/\"\n",
    "MATCHED_TESTING_AUTHORS_DIR = DATA_DIR + \"matched_testing_authors/\"\n",
    "NO_MANO_PARENTS_DIR = DATA_DIR + \"no_mano_parents_results/\"\n",
    "\n",
    "all_subreddits = ['news', 'askreddit', 'worldnews', 'todayilearned', 'askmen', 'movies', 'technology', 'politics', \"adviceanimals\", \"videos\", \"pics\", \"funny\", \"wtf\", \"gaming\"]\n",
    "all_data_liwc = pd.read_csv(f\"{LIWC_DIR}/all_subreddit_data_liwc_2024_14_subreddits.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41de57e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_liwc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561c4fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_liwc(subreddits):\n",
    "    dfs = []\n",
    "    for subreddit in subreddits:\n",
    "        all_data = pd.read_pickle(f\"{PRE_LIWC_DIR}/{subreddit}_all_subreddit_data_pre_liwc.pkl\").rename(columns={\"body\": \"utterance\"})\n",
    "        all_data.to_csv(f\"{subreddit}_all_subreddit_data_pre_liwc.csv\")\n",
    "        print(all_data.shape)\n",
    "        dfs.append(all_data)\n",
    "    mega = pd.concat(dfs)\n",
    "    print(mega.shape)\n",
    "    mega = mega.drop_duplicates(\"id\")\n",
    "    print(mega.shape)\n",
    "    mega.to_csv(f\"{LIWC_DIR}/all_subreddit_data_pre_liwc_2024_14_subreddits.csv\")\n",
    "\n",
    "# prepare_for_liwc(all_subreddits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be1902c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_toxicity_dict():\n",
    "    files = [\"hate_speech_march_5_toxicity.csv\", \"worldnews_til_toxicity.csv\", \"hate_speech_politics_technology_toxicity.csv\", \"hate_speech_all_used_data_toxicity.csv\"]\n",
    "    id_to_toxicity = {}\n",
    "    for file in files:\n",
    "        df = pd.read_csv(TOXICITY_DIR + file, index_col=0)\n",
    "        file_dict = df.set_index(\"id\")['toxicity'].to_dict()\n",
    "        for curr_id in file_dict:\n",
    "            id_to_toxicity[curr_id] = file_dict[curr_id]\n",
    "    return id_to_toxicity\n",
    "\n",
    "\n",
    "def add_toxicity_values(df):\n",
    "    d = get_toxicity_dict()\n",
    "    df['toxicity'] = df['id'].progress_apply(lambda x: d.get(x, np.nan))\n",
    "    print(df['toxicity'].isna().sum())\n",
    "    print(df['toxicity'].shape)\n",
    "    print(df['toxicity'].isna().sum() / df['toxicity'].shape[0])\n",
    "    df['toxicity'] = df['toxicity'].fillna(0)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9f01c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_from_liwc(subreddit):\n",
    "    all_data = pd.read_csv(f\"{PRE_LIWC_DIR}/{subreddit}_all_subreddit_data_pre_liwc.csv\", index_col=0)\n",
    "    liwc_columns = all_data_liwc.columns[len(all_data.columns):].tolist()\n",
    "    all_data_liwc.columns = all_data.columns.tolist() + liwc_columns\n",
    "    curr_sub_liwc = all_data.merge(all_data_liwc[['id'] + liwc_columns], on=\"id\")\n",
    "    assert curr_sub_liwc.shape[0] == all_data.shape[0]\n",
    "    curr_sub_liwc = add_toxicity_values(curr_sub_liwc)\n",
    "    return all_data, curr_sub_liwc\n",
    "\n",
    "\n",
    "def load_features(og_data, liwc_data):\n",
    "    functional_syntactic_features = ['WC', 'WPS', 'Sixltr', 'Dic', 'function', 'pronoun', 'ppron', 'i', 'we',\n",
    "                                     'you', 'shehe', 'they', 'ipron', 'article','prep', 'auxverb', 'conj', 'negate', \n",
    "                                     'interrog', 'number', 'quant', 'AllPunc', 'Period', 'Comma', 'Colon',\n",
    "                                     'SemiC', 'QMark', 'Exclam', 'Dash', 'Quote', 'Apostro', 'Parenth',   'OtherP',\n",
    "                                      \"ttr\"]\n",
    "\n",
    "    uncivil_features = ['valence', 'politeness', 'toxicity']\n",
    "    \n",
    "    liwc_semantic_features = list(set(liwc_data.columns).difference(functional_syntactic_features).difference(og_data.columns).difference(uncivil_features))\n",
    "\n",
    "    top_level_features = [\"AllPunc\", \"Dic\", 'function', 'pronoun', 'ppron', 'affect', 'negemo', 'social', 'cogproc', 'percept', 'bio', 'time', 'drives', 'relativ', 'informal']\n",
    "    \n",
    "    \n",
    "    functional_syntactic_features = [feature for feature in functional_syntactic_features if feature not in top_level_features]\n",
    "    liwc_semantic_features = [feature for feature in liwc_semantic_features if feature not in top_level_features]\n",
    "    \n",
    "    with open(f\"{DATA_DIR}/syntactic_features.txt\", \"w\") as f:\n",
    "        f.write(\"\\t\".join(functional_syntactic_features))\n",
    "    with open(f\"{DATA_DIR}/semantic_features.txt\", \"w\") as f:\n",
    "        f.write(\"\\t\".join(liwc_semantic_features))\n",
    "    with open(f\"{DATA_DIR}/uncivil_features.txt\", \"w\") as f:\n",
    "        f.write(\"\\t\".join(uncivil_features))\n",
    "        \n",
    "    print(f\"# Syntactic Features: {len(functional_syntactic_features)}\")\n",
    "    print(f\"# Semantic Features: {len(liwc_semantic_features)}\")\n",
    "    print(f\"# Uncivil Features: {len(uncivil_features)}\")\n",
    "#     return functional_syntactic_features, liwc_semantic_features\n",
    "\n",
    "\n",
    "\n",
    "def create_datasets(subreddit, liwc_data, to_save):#, semantic_features, to_save):\n",
    "    training_data = liwc_data[liwc_data['is_training'] == 1]\n",
    "    testing_data = liwc_data[liwc_data['is_training'] == 0]\n",
    "\n",
    "    if to_save:\n",
    "        training_data.to_csv(f\"{TRAINING_DIR}/{subreddit}_training_data.csv\", index=False)\n",
    "        testing_data.to_csv(f\"{TESTING_DIR}/{subreddit}_testing_data.csv\", index=False)\n",
    "    \n",
    "    return training_data, testing_data\n",
    "\n",
    "\n",
    "def preprocess_for_r(subreddit_of_interest):\n",
    "    all_data, sub_liwc = load_from_liwc(subreddit_of_interest)\n",
    "    load_features(all_data, sub_liwc)\n",
    "    return create_datasets(subreddit_of_interest, sub_liwc, to_save=True)\n",
    "\n",
    "\n",
    "# for sub in all_subreddits:\n",
    "#     print(sub)\n",
    "#     preprocess_for_r(sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2532ac69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_predicted_data(data_name, style_type, test_data, include_parent_features=False):\n",
    "    predicted_data = pd.read_csv(f\"{PREDICTIONS_DIR}/{data_name}_{style_type}_testing_data_predictions.csv\", index_col=0)\n",
    "    predicted_data.index = predicted_data.index - 1\n",
    "#     print(predicted_data.groupby(\"label\")['probs'].count())\n",
    "#     print(predicted_data.groupby(\"label\")['probs'].mean())\n",
    "    \n",
    "    test_data = test_data.reset_index(drop=True)\n",
    "    test_data['probs'] = predicted_data['probs']\n",
    "    \n",
    "    test_posts = test_data[test_data['is_parent'] == 0]\n",
    "    parent_posts = test_data[test_data['is_parent'] == 1]\n",
    "    \n",
    "    parent_posts = parent_posts.drop_duplicates(\"id\")\n",
    "    parent_posts = parent_posts.set_index(\"id\").loc[test_posts['cropped_id']]\n",
    "    assert parent_posts.shape[0] == test_posts.shape[0]\n",
    "    parent_posts = parent_posts.reset_index()\n",
    "    test_posts = test_posts.reset_index()\n",
    "    \n",
    "    test_posts['parent_author'] = parent_posts['author']\n",
    "    test_posts['parent_probs'] = parent_posts['probs']\n",
    "    test_posts['more_manospheric'] = test_posts['probs'] - test_posts['parent_probs']\n",
    "    \n",
    "    if include_parent_features:\n",
    "        for feature in ['female', 'male', 'toxicity', 'politeness', 'valence']:\n",
    "            test_posts[f\"{feature}_parent\"] = parent_posts[feature]\n",
    "    \n",
    "    validation_data_liwc = test_posts[test_posts['label'] == 2]\n",
    "    test_data_liwc = test_posts[test_posts['label'] != 2]\n",
    "    \n",
    "    return validation_data_liwc, test_data_liwc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7a635e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cohens_d(arr_1, arr_2):\n",
    "    numerator = np.mean(arr_1) - np.mean(arr_2)\n",
    "    denominator = np.sqrt((np.std(arr_1) ** 2 + np.std(arr_2) ** 2)/2)\n",
    "    return np.round(numerator/denominator, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0bc51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_data(validation_data_liwc, test_data_liwc, matched_authors):\n",
    "    validation_agg = validation_data_liwc.groupby(\"author\").mean()\n",
    "    test_agg = test_data_liwc.groupby(\"author\").mean()\n",
    "    manosphere_agg = test_agg[test_agg['label'] == 1]\n",
    "    baseline_agg = test_agg[test_agg['label'] == 0]\n",
    "    \n",
    "    if \"autotldr\" in test_agg.index:\n",
    "        print(\"FOUND BOT\")\n",
    "        manosphere_agg = manosphere_agg[~manosphere_agg.index.isin([\"autotldr\"])]\n",
    "        validation_agg = validation_agg[~validation_agg.index.isin([\"autotldr\"])]\n",
    "        baseline_agg = baseline_agg[~baseline_agg.index.isin([matched_authors['autotldr']])]\n",
    "        \n",
    "        v = validation_data_liwc[~validation_data_liwc['author'].isin([\"autotldr\"])]\n",
    "        print(v.groupby(\"label\")[['author', 'id']].nunique())\n",
    "        \n",
    "        m = test_data_liwc[~test_data_liwc['author'].isin([\"autotldr\", matched_authors['autotldr']])]\n",
    "        print(m.groupby(\"label\")[['author', 'id']].nunique())\n",
    "#         assert baseline_agg.shape[0] == manosphere_agg.shape[0]\n",
    "        print(manosphere_agg.shape)\n",
    "    print(baseline_agg['probs'].mean())\n",
    "    print(manosphere_agg['probs'].mean())\n",
    "    print(validation_agg['probs'].mean())\n",
    "    \n",
    "    return validation_agg, manosphere_agg, baseline_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b1e6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(data_name, style_type, metadata, remove_manospheric_parents):\n",
    "    testing_data = pd.read_csv(f\"{TESTING_DIR}/{data_name}_testing_data.csv\")\n",
    "    validation_data_liwc, test_data_liwc = load_predicted_data(data_name, style_type, testing_data)\n",
    "    \n",
    "        \n",
    "    validation_data, incel_data, control_data = aggregate_data(validation_data_liwc, test_data_liwc, metadata['matched_authors'])\n",
    "    stats, tests = compute_statistics(validation_data, incel_data, control_data, metadata)    \n",
    "    relevant_data = {\"baseline\": control_data, \"manosphere\": incel_data, \"validation\": validation_data}\n",
    "    return pd.DataFrame(stats).T, pd.DataFrame(tests).T, relevant_data\n",
    "\n",
    "def main_wrapper(subreddits, style_type, remove_manospheric_parents=False, remove_responses_to_manospheric_posts=False):\n",
    "    all_stats = []\n",
    "    all_tests = []\n",
    "    all_data = {}\n",
    "    extended_subcultures = [\"Manosphere\"]# + subcultures\n",
    "    \n",
    "    for subreddit in subreddits:\n",
    "        metadata = {}\n",
    "        metadata['subreddit'] = subreddit\n",
    "        metadata[\"matched_authors\"] = pd.read_pickle(f\"{MATCHED_TESTING_AUTHORS_DIR}/{subreddit}_matched_authors.pkl\")\n",
    "        \n",
    "        for subculture in extended_subcultures:\n",
    "            metadata['subculture'] = subculture\n",
    "            data_name = subreddit if subculture == \"Manosphere\" else f\"{subreddit}_{subculture}\"\n",
    "            stats, tests, data = main(data_name, style_type, metadata, remove_manospheric_parents, remove_responses_to_manospheric_posts)\n",
    "            all_stats.append(stats)\n",
    "            all_tests.append(tests)\n",
    "            all_data[subreddit] = data\n",
    "    \n",
    "    return all_stats, all_tests, all_data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def compute_statistics(validation_data, incel_data, control_data, metadata, remove_responses_to_manospheric_posts):\n",
    "    \n",
    "    subreddit = metadata['subreddit']\n",
    "    subculture = metadata['subculture']\n",
    "    matched_authors = metadata['matched_authors']\n",
    "    \n",
    "    # Order the baseline data appropriately\n",
    "    if remove_responses_to_manospheric_posts:\n",
    "        ttest_func = lambda x, y: ttest_ind(x, y)\n",
    "        cohens_d_func = lambda x, y: compute_cohens_d(x, y)\n",
    "    else:\n",
    "        ttest_func = lambda x, y: ttest_rel(x, y)\n",
    "        cohens_d_func = lambda x, y: compute_cohens_d(x - y, np.zeros(len(incel_data['probs'])))\n",
    "        control_data = control_data.loc[[matched_authors[author] for author in incel_data.index]]\n",
    "    \n",
    "    # Compute group-level statistics\n",
    "    descriptive_statistics = {}\n",
    "    for df, title in zip([control_data, incel_data, validation_data], ['Baseline', \"Test\", \"Validation\"]):\n",
    "        descriptive_statistics[(subreddit, subculture, title)] = {\n",
    "                                 \"num_users\": len(df['probs']),\n",
    "                                 \"reply_mano_mean\": np.mean(df['probs']),\n",
    "                                 \"reply_mano_std\": np.std(df['probs']),\n",
    "                                 \"parent_mano_mean\": np.mean(df['parent_probs']),\n",
    "                                 \"parent_mano_std\": np.std(df['parent_probs']),\n",
    "                                 \"more_mano_mean\": np.mean(df['more_manospheric']),\n",
    "                                 \"more_mano_std\": np.std(df['more_manospheric'])}    \n",
    "\n",
    "    # Perform Hypothesis Testing\n",
    "    print(\"Hypothesis Tests\")\n",
    "    hypothesis_test_data = {}\n",
    "  \n",
    "    print(\"\\n Test 1: Manospheric Replies vs. On-Manosphere Replies\")\n",
    "    print(\"Matched pairs t-test\")\n",
    "    tstat, p_val = ttest_func(incel_data['probs'], validation_data['probs'])\n",
    "    cohens_d = cohens_d_func(incel_data['probs'], validation_data['probs'])\n",
    "    hypothesis_test_data[(subreddit, subculture, \"compare_to_validation\")] = {\"statistic\": tstat, \"p_val\": p_val, \"cohens_d\": cohens_d}\n",
    "    print(f\"Statistic: {np.round(tstat, 2)}\\np-value: {np.round(p_val, 6)}\\nCohen's D: {np.round(cohens_d, 2)}\")\n",
    "    \n",
    "    \n",
    "    print(\"\\n Test 2: Manospheric Replies vs. Control Replies\")\n",
    "    print(\"Matched pairs t-test\")\n",
    "    tstat, p_val = ttest_func(incel_data['probs'], control_data['probs'])\n",
    "    cohens_d = cohens_d_func(incel_data['probs'].to_numpy(),  control_data['probs'].to_numpy())\n",
    "    hypothesis_test_data[(subreddit, subculture, \"compare_to_control\")] = {\"statistic\": tstat, \"p_val\": p_val, \"cohens_d\": cohens_d}\n",
    "    print(f\"Statistic: {np.round(tstat, 2)}\\np-value: {np.round(p_val, 6)}\\nCohen's D: {np.round(cohens_d, 2)}\")\n",
    "    \n",
    "    \n",
    "    print(\"\\n Test 3: Manospheric Parents vs. Control Parents\")\n",
    "    print(\"Matched pairs t-test\")\n",
    "    tstat, p_val = ttest_func(incel_data['parent_probs'], control_data['parent_probs'])\n",
    "    cohens_d = cohens_d_func(incel_data['parent_probs'].to_numpy(), control_data['parent_probs'].to_numpy())\n",
    "    hypothesis_test_data[(subreddit, subculture, \"compare_parents\")] = {\"statistic\": tstat, \"p_val\": p_val, \"cohens_d\": cohens_d}\n",
    "    print(f\"Statistic: {np.round(tstat, 2)}\\np-value: {np.round(p_val, 6)}\\nCohen's D: {np.round(cohens_d, 2)}\")\n",
    "    \n",
    "    \n",
    "#     print(\"\\n Test 1: Manospheric Replies vs. On-Manosphere Replies\")\n",
    "#     print(\"Matched pairs t-test\")\n",
    "#     tstat, p_val = ttest_rel(incel_data['probs'], validation_data['probs'])\n",
    "#     cohens_d = compute_cohens_d(incel_data['probs']- validation_data['probs'], np.zeros(len(incel_data['probs'])))\n",
    "#     hypothesis_test_data[(subreddit, subculture, \"compare_to_validation\")] = {\"statistic\": tstat, \"p_val\": p_val, \"cohens_d\": cohens_d}\n",
    "#     print(f\"Statistic: {np.round(tstat, 2)}\\np-value: {np.round(p_val, 6)}\\nCohen's D: {np.round(cohens_d, 2)}\")\n",
    "    \n",
    "    \n",
    "#     print(\"\\n Test 2: Manospheric Replies vs. Control Replies\")\n",
    "#     print(\"Matched pairs t-test\")\n",
    "#     tstat, p_val = ttest_rel(incel_data['probs'], control_data['probs'])\n",
    "#     cohens_d = compute_cohens_d(incel_data['probs'].to_numpy() - control_data['probs'].to_numpy(), np.zeros(len(incel_data['probs'])))\n",
    "#     hypothesis_test_data[(subreddit, subculture, \"compare_to_control\")] = {\"statistic\": tstat, \"p_val\": p_val, \"cohens_d\": cohens_d}\n",
    "#     print(f\"Statistic: {np.round(tstat, 2)}\\np-value: {np.round(p_val, 6)}\\nCohen's D: {np.round(cohens_d, 2)}\")\n",
    "    \n",
    "    \n",
    "#     print(\"\\n Test 3: Manospheric Parents vs. Control Parents\")\n",
    "#     print(\"Matched pairs t-test\")\n",
    "#     tstat, p_val = ttest_rel(incel_data['parent_probs'], control_data['parent_probs'])\n",
    "#     cohens_d = compute_cohens_d(incel_data['parent_probs'].to_numpy() - control_data['parent_probs'].to_numpy(), np.zeros(len(incel_data['parent_probs'])))\n",
    "#     hypothesis_test_data[(subreddit, subculture, \"compare_parents\")] = {\"statistic\": tstat, \"p_val\": p_val, \"cohens_d\": cohens_d}\n",
    "#     print(f\"Statistic: {np.round(tstat, 2)}\\np-value: {np.round(p_val, 6)}\\nCohen's D: {np.round(cohens_d, 2)}\")\n",
    "    \n",
    "\n",
    "    return descriptive_statistics, hypothesis_test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65437a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# subreddit_of_interest = 'askmen'\n",
    "# style_type = 'all_features_reduced'\n",
    "for style_type in ['emnlp_data']:\n",
    "    all_stats, all_tests, all_mano_data = main_wrapper(all_subreddits, style_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6ca8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create two subplots and unpack the output array immediately\n",
    "def plot_subreddit_data(validation_data, incel_data, control_data, style_type, plot_metadata):\n",
    "    \n",
    "    plt.rcParams.update({\"font.size\": 15})\n",
    "    subreddit = plot_metadata['subreddit']\n",
    "    subculture = plot_metadata['subculture']\n",
    "    \n",
    "    fontsize = plot_metadata['fontsize']\n",
    "    figsize_x = plot_metadata['figsize_x'] \n",
    "    figsize_y = plot_metadata['figsize_y']\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, sharey=True, figsize=(figsize_x, figsize_y))\n",
    "\n",
    "    # Plot 1: Reply Results\n",
    "    sns.kdeplot(control_data[\"probs\"], label=f\"Baseline on r/{subreddit}\", ax=ax1, linestyle=\"dashed\", legend=False, color=\"#f8766d\")\n",
    "    sns.kdeplot(validation_data[\"probs\"], label=f\"{subculture} in {subculture}\", ax=ax1, linestyle=\"dashdot\", legend=False, color=\"#00bfc4\")\n",
    "    sns.kdeplot(incel_data[\"probs\"], label=f\"{subculture} on r/{subreddit}\", ax=ax1, linestyle=\"solid\", legend=False, color=\"green\")    \n",
    "\n",
    "    \n",
    "    # Plot 2: Parent Results\n",
    "    sns.kdeplot(control_data[\"parent_probs\"], label=f\"Baseline on r/{subreddit}\", ax=ax2, linestyle=\"dashed\", legend=False, color=\"#f8766d\")\n",
    "#     sns.kdeplot(validation_data[\"parent_probs\"], label=f\"{subculture} in {subculture}\", ax=ax2, linestyle=\"dashdot\", legend=False, color=\"#00bfc4\")\n",
    "    sns.kdeplot(incel_data[\"parent_probs\"], label=f\"{subculture} on r/{subreddit}\", ax=ax2, linestyle=\"solid\", legend=False, color=\"green\")    \n",
    "\n",
    "    \n",
    "    \n",
    "#     # Plot 3: Reply - Parent Results\n",
    "#     sns.kdeplot(control_data[\"more_manospheric\"], label=f\"{subreddit} on {subreddit}\", ax=ax2, linestyle=\"dashed\", legend=False, color=\"#f8766d\")\n",
    "#     sns.kdeplot(incel_data[\"more_manospheric\"], label=f\"{subculture} on r/{subreddit}\", ax=ax2, linestyle=\"solid\", legend=False, color=\"green\")\n",
    "    \n",
    "    ax1.set_xlabel(f\"Reply {subculture}-ness\", fontsize=fontsize, labelpad=20)\n",
    "    ax1.set_xlim(0.15, 0.85)\n",
    "    ax1.set_ylim(0, 12)\n",
    "    ax2.set_xlabel(f\"Parent {subculture}-ness\", fontsize=fontsize, labelpad=20)\n",
    "    \n",
    "#     ax2.set_xlabel(f\"Reply {subculture}-ness - \\nParent {subculture}-ness\\n\", fontsize=fontsize, labelpad=20)\n",
    "    ax1.set_ylabel(\"Density\", fontsize=fontsize, labelpad=20)\n",
    "    ax2.set_xlim(0.15, 0.85)\n",
    "    \n",
    "#     ax2.set_xlim(-0.45, 0.45)\n",
    "    ax2.set_ylim(0, 12)\n",
    "    ax1.tick_params(axis='both', which='major', labelsize=fontsize)\n",
    "    ax2.tick_params(axis='both', which='major', labelsize=fontsize)\n",
    "\n",
    "    handles, labels = ax1.get_legend_handles_labels()\n",
    "    fig.legend(handles, labels, loc='upper left', bbox_to_anchor=(0.2, 0.9), fancybox=True, shadow=True, fontsize=25)    \n",
    "    fig.suptitle(f\"{subculture}-ness Distributions on \\n r/{subreddit} (n={incel_data.shape[0]})\", fontsize=fontsize+10, y=1.02)\n",
    "    plt.savefig(f\"{subreddit}_{subculture}ness_{style_type}.png\", bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e96a33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create two subplots and unpack the output array immediately\n",
    "def plot_shifting_data(subreddit_data, group_type, plot_metadata):\n",
    "    \n",
    "    fontsize = plot_metadata['fontsize']\n",
    "    figsize_x = plot_metadata['figsize_x'] \n",
    "    figsize_y = plot_metadata['figsize_y']\n",
    "    plt.rcParams.update({\"font.size\": fontsize})\n",
    "    \n",
    "    sub_to_name = {\n",
    "        \"askreddit\": \"AskReddit\",\n",
    "        \"news\": \"News\",\n",
    "        \"worldnews\": \"WorldNews\",\n",
    "        \"todayilearned\": \"TodayILearned\",\n",
    "        \"askmen\": \"AskMen\",\n",
    "        \"movies\": \"Movies\",\n",
    "        \"politics\": \"Politics\",\n",
    "        \"technology\": \"Technology\",\n",
    "        \"adviceanimals\": \"AdviceAnimals\",\n",
    "        \"wtf\": \"wtf\",\n",
    "        \"videos\": \"Videos\",\n",
    "        \"pics\": \"Pics\",\n",
    "        \"funny\": \"Funny\",\n",
    "        \"gaming\": \"Gaming\"      \n",
    "        \n",
    "    }\n",
    "    \n",
    "    fig, axes = plt.subplots(7, 2, sharey=True, sharex=True, figsize=(figsize_x, figsize_y))\n",
    "    axes = axes.reshape(-1, )\n",
    "    \n",
    "    if group_type == \"reply\":\n",
    "        field = \"probs\"\n",
    "        plt.setp(axes, xlim=[0.15, 0.85], ylim=[0, 9], yticks=[0, 2, 4, 6, 8])\n",
    "    elif group_type == 'parent':\n",
    "        field = 'parent_probs'\n",
    "        plt.setp(axes, xlim=[0.2, 0.8], ylim=[0, 15.5], yticks=[0, 2, 4, 6, 8, 10, 12, 14])\n",
    "        \n",
    "    for subreddit, ax in zip(subreddit_data, axes):\n",
    "        \n",
    "        control_data = subreddit_data[subreddit]['baseline']\n",
    "        incel_data = subreddit_data[subreddit]['manosphere']\n",
    "        \n",
    "        sns.kdeplot(control_data[field], label=f\"Baseline\", ax=ax, linestyle=\"dashed\", legend=False, color=\"#cc0000\")\n",
    "        sns.kdeplot(incel_data[field], label=f\"Mano (outside Mano)\", ax=ax, linestyle=\"solid\", legend=False, color=\"green\")    \n",
    "        \n",
    "        if group_type == \"reply\":          \n",
    "            validation_data = subreddit_data[subreddit]['validation']\n",
    "            sns.kdeplot(validation_data[field], label=f\"Mano (on Mano)\", ax=ax, linestyle=\"dashdot\", legend=False, color=\"#1155cc\")\n",
    "\n",
    "        ax.set_title(f\"r/{sub_to_name[subreddit]} (n={control_data.shape[0]})\")\n",
    "        ax.set(xlabel=None)\n",
    "        ax.set(ylabel=None)\n",
    "    \n",
    "    fig.supxlabel(f'{group_type.title()} Manosphericness', fontsize=40, y=0.07)\n",
    "    fig.supylabel('Density', fontsize=40, x=0.05)\n",
    "\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "#     fig.legend(handles, labels, loc='upper right', bbox_to_anchor=(1, 0.85), fancybox=True, shadow=True, fontsize=20)    \n",
    "#     fig.suptitle(f\"{subculture}-ness Distributions on \\n r/{subreddit} (n={incel_data.shape[0]})\", fontsize=fontsize+10, y=1.02)\n",
    "    plt.savefig(f\"{group_type}.png\", bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d39da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create two subplots and unpack the output array immediately\n",
    "def plot_shifting_data_three(subreddit_data, group_type, plot_metadata):\n",
    "    \n",
    "    fontsize = plot_metadata['fontsize']\n",
    "    figsize_x = plot_metadata['figsize_x'] \n",
    "    figsize_y = plot_metadata['figsize_y']\n",
    "    plt.rcParams.update({\"font.size\": fontsize})\n",
    "    \n",
    "    sub_to_name = {\n",
    "        \"askreddit\": \"AskReddit\",\n",
    "        \"news\": \"News\",\n",
    "        \"worldnews\": \"WorldNews\",\n",
    "        \"todayilearned\": \"TodayILearned\",\n",
    "        \"askmen\": \"AskMen\",\n",
    "        \"movies\": \"Movies\",\n",
    "        \"politics\": \"Politics\",\n",
    "        \"technology\": \"Technology\",\n",
    "        \"adviceanimals\": \"AdviceAnimals\",\n",
    "        \"wtf\": \"wtf\",\n",
    "        \"videos\": \"Videos\",\n",
    "        \"pics\": \"Pics\",\n",
    "        \"funny\": \"Funny\",\n",
    "        \"gaming\": \"Gaming\"      \n",
    "        \n",
    "    }\n",
    "    \n",
    "    fig, axes = plt.subplots(5, 3, sharey=True, sharex=True, figsize=(figsize_x, figsize_y))\n",
    "    axes = axes.reshape(-1, )\n",
    "#     [axes]\n",
    "    \n",
    "    if group_type == \"reply\":\n",
    "        field = \"probs\"\n",
    "        plt.setp(axes, xlim=[0.15, 0.85], ylim=[0, 9], yticks=[0, 2, 4, 6, 8])\n",
    "    elif group_type == 'parent':\n",
    "        field = 'parent_probs'\n",
    "        plt.setp(axes, xlim=[0.2, 0.8], ylim=[0, 15.5], yticks=[0, 2, 4, 6, 8, 10, 12, 14])\n",
    "        \n",
    "    for subreddit, ax in zip(subreddit_data, axes):\n",
    "        \n",
    "        control_data = subreddit_data[subreddit]['baseline']\n",
    "        incel_data = subreddit_data[subreddit]['manosphere']\n",
    "        \n",
    "        sns.kdeplot(control_data[field], label=f\"Baseline Authors\", ax=ax, linestyle=\"dashed\", legend=False, color=\"#cc0000\")\n",
    "        sns.kdeplot(incel_data[field], label=f\"Manospheric Authors\\n(Outside Manosphere)\", ax=ax, linestyle=\"solid\", legend=False, color=\"green\")    \n",
    "        \n",
    "        if group_type == \"reply\":          \n",
    "            validation_data = subreddit_data[subreddit]['validation']\n",
    "            sns.kdeplot(validation_data[field], label=f\"Manospheric Authors\\n(Inside Manosphere)\", ax=ax, linestyle=\"dashdot\", legend=False, color=\"#1155cc\")\n",
    "\n",
    "        ax.set_title(f\"r/{sub_to_name[subreddit]} (n={control_data.shape[0]})\")\n",
    "        ax.set(xlabel=None)\n",
    "        ax.set(ylabel=None)\n",
    "    \n",
    "    fig.supxlabel(f'Manosphericness', fontsize=30, y=0.07)\n",
    "    fig.supylabel('Density', fontsize=30, x=0.07)\n",
    "\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    fig.legend(handles, labels, loc='upper right', bbox_to_anchor=(0.95, 0.24), fancybox=True, shadow=True, fontsize=20)\n",
    "    fig.delaxes(axes[-1])\n",
    "#     fig.suptitle(f\"{subculture}-ness Distributions on \\n r/{subreddit} (n={incel_data.shape[0]})\", fontsize=fontsize+10, y=1.02)\n",
    "    plt.savefig(f\"{group_type}.png\", bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637f8169",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "metadata = {\n",
    "        \"fontsize\": 20, \n",
    "        \"figsize_x\": 15,\n",
    "        \"figsize_y\": 20\n",
    "    }\n",
    "plot_shifting_data_three(all_mano_data, \"reply\", metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08d245b",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = {\n",
    "        \"fontsize\": 20, \n",
    "        \"figsize_x\": 15,\n",
    "        \"figsize_y\": 4\n",
    "    }\n",
    "plot_shifting_data_three({key: all_mano_data[key] for key in ['worldnews', 'funny', 'askreddit']}, \"reply\", metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84d6679",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All testing data\n",
    "subreddit_to_training_data = {}\n",
    "for subreddit in all_subreddits:\n",
    "    training_data = pd.read_csv(f\"{TRAINING_DIR}/{subreddit}_training_data.csv\")\n",
    "    subreddit_to_training_data[subreddit] = training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200321e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddit_to_train_num = {subreddit: {} for subreddit in all_subreddits}\n",
    "for subreddit in all_subreddits:\n",
    "    curr = subreddit_to_training_data[subreddit].groupby(\"label\")[['author', 'id']].nunique()\n",
    "    num_baseline = f\"{curr.loc[0]['id']} posts/{curr.loc[0]['author']} authors\"\n",
    "    num_mano = f\"{curr.loc[1]['id']} posts/{curr.loc[1]['author']} authors\"\n",
    "    subreddit_to_train_num[subreddit]['M'] = num_mano\n",
    "    subreddit_to_train_num[subreddit]['B'] = num_baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202c5538",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(subreddit_to_train_num).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5fcf6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All testing data\n",
    "subreddit_to_test_data = {}\n",
    "for subreddit in all_subreddits:\n",
    "    testing_data = pd.read_csv(f\"{TESTING_DIR}/{subreddit}_testing_data.csv\")\n",
    "    subreddit_to_test_data[subreddit] = testing_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70c5fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddit_to_test_num = {subreddit: {} for subreddit in all_subreddits}\n",
    "for subreddit in all_subreddits:\n",
    "    curr = subreddit_to_test_data[subreddit].groupby(\"label\")[['author', 'id']].nunique()\n",
    "    num_baseline = f\"{curr.loc[0]['id']} posts/{curr.loc[0]['author']} authors\"\n",
    "    num_mano = f\"{curr.loc[1]['id']} posts/{curr.loc[1]['author']} authors\"\n",
    "    num_validation = f\"{curr.loc[2]['id']} posts/{curr.loc[2]['author']} authors\"\n",
    "    subreddit_to_test_num[subreddit]['M (in Manosphere)'] = num_validation\n",
    "    subreddit_to_test_num[subreddit]['M (on $S$)'] = num_mano\n",
    "    subreddit_to_test_num[subreddit]['B (on $S$)'] = num_baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e3e885",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(subreddit_to_test_num).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1371e94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All testing data\n",
    "subreddit_to_predicted_data = {}\n",
    "subreddit_to_validation_data = {}\n",
    "for subreddit in all_subreddits:\n",
    "    validation_data, predicted_data = load_predicted_data(subreddit, \"emnlp_data\", subreddit_to_test_data[subreddit])\n",
    "    subreddit_to_predicted_data[subreddit] = predicted_data\n",
    "    subreddit_to_validation_data[subreddit] = validation_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea51b90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddit_to_validation_data['askreddit'].sort_values(by='probs').tail(200).iloc[4]#['utterance']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db832d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = pd.read_csv(TRAINING_DIR + 'full_sample_14_subreddits_training_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71559156",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data.groupby(\"label\")[['female', 'male', 'shehe', 'you', 'toxicity', 'Sixltr', 'politeness']].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6864b1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_to_gender_counts = {}\n",
    "for subreddit in all_subreddits:\n",
    "    curr = subreddit_to_predicted_data[subreddit].groupby(\"label\")[['female', 'male', 'shehe']].mean()\n",
    "    diffs = curr.loc[1] - curr.loc[0]\n",
    "#     curr = pd.DataFrame(pd.concat({subreddit: diffs}, names=['Subreddit']))\n",
    "    sub_to_gender_counts[subreddit] = diffs\n",
    "pd.DataFrame(sub_to_gender_counts).round(3).mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3ce584",
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptive_statistics = pd.concat(all_stats)\n",
    "descriptive_statistics.index = descriptive_statistics.index.set_names(['subreddit', 'subculture', 'group'])\n",
    "num_users = pd.DataFrame(descriptive_statistics['num_users'])\n",
    "num_users = num_users.droplevel(2, axis=0).drop_duplicates()\n",
    "\n",
    "hypothesis_tests = pd.concat(all_tests)\n",
    "hypothesis_tests.index = hypothesis_tests.index.set_names(['subreddit', 'subculture', 'test'])\n",
    "hypothesis_tests =  hypothesis_tests.combine_first(num_users)\n",
    "\n",
    "global_results = hypothesis_tests[hypothesis_tests.index.get_level_values(1) == \"Manosphere\"]\n",
    "\n",
    "global_results.xs(\"compare_to_control\", level=2)[['statistic', \"p_val\", \"cohens_d\"]].loc[all_subreddits]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca8b77f",
   "metadata": {},
   "source": [
    "# Feature Shifting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866f0b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_test_data = []\n",
    "for subreddit in all_subreddits:\n",
    "    testing_data = pd.read_csv(f\"{TESTING_DIR}/{subreddit}_testing_data.csv\")\n",
    "    validation_data_liwc, test_data_liwc = load_predicted_data(subreddit, 'emnlp_data', testing_data, include_parent_features=True)\n",
    "    all_test_data.append(test_data_liwc)\n",
    "\n",
    "all_test_data_df = pd.concat(all_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79148ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_test_data_df = all_test_data_df[['label', 'toxicity', 'politeness', 'valence', 'female', 'male', 'toxicity_parent', 'politeness_parent', 'valence_parent', 'female_parent', 'male_parent']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6231b485",
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in all_test_data_df.columns[1:]:\n",
    "    col = all_test_data_df[feature]\n",
    "    all_test_data_df[feature] = (col - col.mean())/col.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28f94a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_test_data_df.to_csv(f\"{TESTING_DIR}/feature_switching_emnlp_data.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
